---
title: "Assignment-3"
author: "Akshaya Mamidipalli"
date: "2024-03-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(caret)
library(ISLR)
library(e1071)
```

```{r}
Bank_Data <- read.csv("C:/Users/mamid/Downloads/UniversalBank.csv")
```

```{r}
str(Bank_Data)
```
```{r}
Mydata <- Bank_Data %>% select(Age, Experience, Income, Family, 
CCAvg, Education, Mortgage, Personal.Loan, Securities.Account, CD.Account, 
Online, CreditCard) 
Mydata$CreditCard <- as.factor(Mydata$CreditCard) 
Mydata$Personal.Loan <- as.factor((Mydata$Personal.Loan)) 
Mydata$Online <- as.factor(Mydata$Online)
```

```{r}
selected.var <- c(8,11,12) 
set.seed(23)
```

```{r}
Train_Index <-  createDataPartition(Mydata$Personal.Loan, p=0.60,list=FALSE) 
Train_Data <-  Mydata[Train_Index,selected.var] 
Validation_Data <- Mydata[-Train_Index,selected.var] 
```

```{r}
attach(Train_Data) 
ftable(CreditCard,Personal.Loan,Online) 
```
```{r}
detach(Train_Data)
```

#B)
```{r}
prop.table(ftable(Train_Data$CreditCard,Train_Data$Online,Train_Data$Personal.Loan),margin=1) 
```

#C)
```{r}
attach(Train_Data) 
ftable(Personal.Loan,Online) 
```

```{r}
ftable(Personal.Loan,CreditCard)
```
```{r}
detach(Train_Data)
```

#D)
```{r}
prop.table(ftable(Train_Data$Personal.Loan,Train_Data$CreditCard),margin=1)
```
```{r}
prop.table(ftable(Train_Data$Personal.Loan,Train_Data$Online),margin=1)
```
A proportion pivot table that can help with the answer to question D is displayed by the code above.
D(i) 92/288 = 0.3194 or 31.94% 
D(ii) 167/288 = 0.5798 or 57.986%
D(iii) 288 divided by total count from table (3000) = 0.096 or 9.6%
D(iV) 812/2712 = 0.2994 or 29.94% 
D(V) 1624/2712 = 0.5988 or 59.88% 
D(Vi)2712  divided by 3000 = 0.904 or 90.4% 

#E)
E)Naive Bayes calculation (0.3194 * 0.5798 * 0.096)/[(0.3194 * 0.5798 * 0.096)+(0.2994 * 0.5988 * 0.904)] = 0.0988505642823701 or 9.885% 

#F)
B utilizes a straightforward calculation predicated on a count, while E applies probability to every count. Since it turns out, B is more exact while E is better for wide generalization.

#G)

```{r}
naivebayes <- naiveBayes(Personal.Loan ~ ., data = Train_Data) 
naivebayes 
```
We can quickly calculate P(LOAN=1|CC=1,Online=1) without using the Naive Bayes model by using the pivot table constructed in step B, but it's also easy to see how you're computing P(LOAN=1|CC=1,Online=1) using the Naive Bayes model by using the two tables developed in step C. While it is less than the probability manually determined in step E, the Naive Bayes model estimates the exact same probability as the earlier methods. This probability is more in line with the figure determined in step B. This might be because we are performing the computations by hand in step E, which allows for error when rounding fractions and produces only a rough estimate.

```{r}
predicted_class <- predict(naivebayes, newdata = Train_Data) 
confusionMatrix(predicted_class, Train_Data$Personal.Loan) 
```





```{r}
predicted_prob <- predict(naivebayes, newdata=Validation_Data, type="raw") 
predicted_class <- predict(naivebayes, newdata = Validation_Data) 
confusionMatrix(predicted_class, Validation_Data$Personal.Loan) 
```
```{r}
library(pROC) 
```
```{r}
roc(Validation_Data$Personal.Loan,predicted_prob[,1])
```
```{r}
plot.roc(Validation_Data$Personal.Loan,predicted_prob[,1],print.thres="best")
```
This demonstrates the way the model's accuracy could be enhanced by reducing sensitivity to 0.495 and raising specificity to 0.576 by establishing a threshold of 0.906.
